{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face\n",
    "Huggign Face is an american company which develops tools for building application using machine learning. It is a website where people can share their ML models. It is most notably known for it’s transformers library which is used to perform different NLP tasks. \n",
    "\n",
    "This is a python notebook based on this playlist - https://www.youtube.com/playlist?list=PLo2EIpI_JMQvWfQndUesu0nPBAtZ9gP1o "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pipeline Function\n",
    "The pipeline function is the most high level API that Hugging Face library offers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9974707365036011}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\"I hade a healthy breakfast this morning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about trandformers library',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.7521424889564514, 0.1648690402507782, 0.08298840373754501]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier(\n",
    "    \"This is a course about trandformers library\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pipeline with custom library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 762/762 [00:00<00:00, 60.1kB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 353M/353M [00:30<00:00, 11.7MB/s] \n",
      "c:\\Users\\pradhumn\\anaconda3\\envs\\vicuna\\lib\\site-packages\\transformers\\modeling_utils.py:402: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  with safe_open(checkpoint_file, framework=\"pt\") as f:\n",
      "c:\\Users\\pradhumn\\anaconda3\\envs\\vicuna\\lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "c:\\Users\\pradhumn\\anaconda3\\envs\\vicuna\\lib\\site-packages\\torch\\storage.py:899: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = cls(wrap_storage=untyped_storage)\n",
      "c:\\Users\\pradhumn\\anaconda3\\envs\\vicuna\\lib\\site-packages\\safetensors\\torch.py:99: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  with safe_open(filename, framework=\"pt\", device=device) as f:\n",
      "Downloading (…)neration_config.json: 100%|██████████| 124/124 [00:00<00:00, 9.59kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 4.05MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 2.44MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 4.90MB/s]\n",
      "c:\\Users\\pradhumn\\anaconda3\\envs\\vicuna\\lib\\site-packages\\transformers\\generation\\utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'India is a very strong player in the NBA Draft, and their team has tremendous potential at this point in time.\\n\\n\\n\\n\\n\\n\\n'},\n",
       " {'generated_text': 'India is a very good choice and is a very attractive value for investors and companies to invest in. However it could have been different, especially in India'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model='distilgpt2')\n",
    "generator(\n",
    "    \"India is a very\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the save_pretrained() method to save the configs, model weights and vocabulary:\n",
    "\n",
    "classifier.save_pretrained('/some/directory')  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different pipelines\n",
    "Text classification\n",
    "<br>\n",
    "Zero-shot classification\n",
    "<br>\n",
    "Text Generation\n",
    "<br>\n",
    "Text completion\n",
    "<br>\n",
    "Token classification\n",
    "<br>\n",
    "Question answering\n",
    "<br>\n",
    "Summatization\n",
    "<br>\n",
    "Translation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning is basically finetuning a existing model. When we train a model from scratch we randomly initialize the weights of the model. In fine tuning/ transfer learning we use the weights of some pretrained model. \n",
    "\n",
    "Transfer learning has been succesfully been used in Image datasets but it is fairly new in NLP tasks. It works great on NLP tasks as well but it has a problem of being biased to the previous model. If a model is trained more on US data then the fine tuned has more bais towards the US english linguistic characteristics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Architecture\n",
    "\n",
    "The Transformer architecture consist of 2 parts, encoder and decoder.\n",
    "Both the encoder and decoder can run as independent components or can be combined together."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Encoder** is bi-directional model in the sense that when generating a vector for a word it takes context from the previous as well as the next word. It uses the self attention mechanism. It outputs the one vector for one input word \n",
    "<be>\n",
    "Encoder examples - BERT, RoBERTa, ALBERT\n",
    "<br>\n",
    "Encoder are best for extracting meaning information, NLU - Natural Language Understanding, Sequence classification (sentiment analysis), question answering, masked language modeling "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Decoder** is a uni directional model in the sense that when predicting the next word it takes context only from the previously generated output.The output generated from the previous imput is added to the new input using auto-regressive method. It uses masked self-attention mechanism. It can generate many words from a given input sequence. \n",
    "<br>\n",
    "Decoder Examples - GPT-2, GPT Neo\n",
    "<br>\n",
    "Decoders are best for Natural Language generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining both are best for many-to-many tasks.<br>Weights are not necessarily shared between encoders and decoders.<br>Input distribution is different form output distribution.<br>Best for Translation tasks where we have to understand the meaning of the sentence to generate output or summarization."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happens inside pipeline function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline consists of 3 stages \n",
    "\n",
    "**Tokenizer** -> **Model** -> **Postprocessing**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw text (adding special tokens for start and end) -> **Tokenizer** -> Input ID's [100, 4054, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
       "          2607,  2026,  2878,  2166,   102],\n",
       "        [  101,  1045,  5223,  2023,  2061,  2172,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life\",\n",
    "    \"I hate this so much\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, return_tensors=\"pt\") #pt = pytorch\n",
    "inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input ID's [100, 4054, ...] -> **Model** -> Logits [-4.3343, 4.4343]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertModel: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 15, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape) # returns [batch size, sequence length, hidden size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4683,  1.5105],\n",
      "        [ 4.2141, -3.4158]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)\n",
    "print(outputs.logits)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logits [-4.3343, 4.4343] -> **Postprocessing** -> Predictions [Positive : 99%, Negative : 0.11%]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.8393e-02, 9.5161e-01],\n",
      "        [9.9951e-01, 4.8549e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vicuna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
